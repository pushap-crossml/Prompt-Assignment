# Prompting Assignment

This repository contains a lightweight Python-based project that integrates **Google Gemini 3 Flash** to study controlled text generation. The primary goal of the project is to experiment with prompt engineering strategies and generation parameters (*temperature, top_p, top_k, max_tokens*) in order to understand how they influence output quality and optimization.

---

## üìå Project Overview

This project is designed as a hands-on exploration of prompt engineering. It focuses on evaluating how different prompting techniques and parameter configurations affect the responses generated by a large language model.

### Prompt Experiments Included:

* General text generation using varied prompt styles
* Few-shot prompting to guide model behavior using examples
* Role-based prompting to control tone and perspective
* Context-based prompting to improve relevance and coherence
* Self-consistency prompting for more reliable outputs
* Chain-of-thought prompting to enhance step-by-step reasoning
* Tree-of-thought prompting to explore multiple reasoning paths

---

## üõ†Ô∏è Tech Stack

* **Language:** Python 3.10+
* **LLM:** Google Gemini (Gemini 3 Flash)
* **SDK:** google-genai

---

## ‚öôÔ∏è Installation

### 1Ô∏è‚É£ Clone the repository

```bash
git clone https://github.com/sujeet-crossml/Prompting_Assignment.git
cd Prompting_Assignment
```

### 2Ô∏è‚É£ Create and activate a virtual environment (recommended)

```bash
python -m venv venv
source venv/bin/activate
```

### 3Ô∏è‚É£ Install dependencies

```bash
pip install google-genai python-dotenv
```

---

## üîê API Key Setup

To run the project, configure your Gemini API key by replacing `API-KEY` in the code:

```python
client = genai.Client(gemini_api_key="YOUR_API_KEY")
```

---

## ‚ñ∂Ô∏è Usage

### 1Ô∏è‚É£ Text Generation Experiment

The main script demonstrates how different prompts and parameter values impact the generated text.

```bash
python main.py
```

---

## üß† Prompts Used

* All prompt templates are defined in the `prompt.py` file
* Prompts can be tested individually to observe output variations
* Each prompt highlights a specific prompt engineering technique

---

## ‚öôÔ∏è Parameters Tuned

* `temperature` ‚Äì controls randomness and creativity
* `top_p` ‚Äì nucleus sampling for probability-based token selection
* `top_k` ‚Äì limits token choices to the top-k most likely options
* `max_output_tokens` ‚Äì restricts the length of generated responses

---

## üß™ Learning Outcomes

* Gain practical insight into how **temperature** affects creativity
* Understand the difference between **top_p** and **top_k** sampling
* Develop hands-on experience with **prompt engineering techniques**
* Compare reasoning quality and structure across multiple prompt styles

---
